{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Nlp_project.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNQ2EG6qllEFM1k4/rVEXCX",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sridas123/CS6320/blob/master/Nlp_project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E6kLtD06jcyv",
        "colab_type": "text"
      },
      "source": [
        "This snippet encodes the string labels with integers, feature vectors with integers from the vocabulary"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KJbASDenjJIG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\"encode the labels with integers\"\"\"\n",
        "def label_encode(string):\n",
        "    labeltrain=[]\n",
        "    for i in range(0,len(string)):\n",
        "       if string[i]==\"TRUE\":\n",
        "          labeltrain.append(1)\n",
        "       else:\n",
        "          labeltrain.append(0)   \n",
        "    return labeltrain \n",
        "\n",
        "    \"\"\"Integer encode a text\"\"\"\n",
        "def integer_encode(string,vocab_dict,out_of_bag_idx):\n",
        "    trains=[] \n",
        "    for i in range(0, len(string)):\n",
        "        ptrain=[]\n",
        "        for word in string[i]:\n",
        "            if word in vocab_dict.keys():\n",
        "               ptrain.append(vocab_dict[word])\n",
        "            else:\n",
        "               ptrain.append(vocab_dict[word+'s'])\n",
        "        ptrain.append(out_of_bag_idx)       \n",
        "        trains.append(ptrain)      \n",
        "    return trains   \n",
        "\n",
        "def pad_sequence_str(string,maxlength):  \n",
        "    ostring=pad_sequences(string,padding='post',maxlen=maxlength)\n",
        "    return ostring"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V6HhSa7akXcv",
        "colab_type": "text"
      },
      "source": [
        "This snippet populates the vocabulary with unique words and their ids"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-KaDY6G9kUhr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\"Populate the vocabulary\"\"\"  \n",
        "def populate_vocab(ptrain,vocab_dict,start=0):\n",
        "    \n",
        "    for i in range(0,len(ptrain)):\n",
        "        str_list=ptrain[i]\n",
        "        for word in str_list:\n",
        "            #print (word)\n",
        "            if (word in vocab_dict.keys()) or (word+'s' in vocab_dict.keys()):\n",
        "               continue  \n",
        "            else:\n",
        "               start=start+1\n",
        "               vocab_dict[word]=start   \n",
        "    return vocab_dict,start \n",
        "\n",
        "\"\"\"Calculate the value for the out of bag words\"\"\"\n",
        "def populate_vocab_test(ptest,vocab_dict,start=0):  \n",
        "\n",
        "    for i in range(0,len(ptest)):\n",
        "        str_list=ptest[i]\n",
        "        for word in str_list:\n",
        "            if (word not in vocab_dict) and (word+'s' not in vocab_dict):\n",
        "                vocab_dict[word]=start   \n",
        "    return vocab_dict             "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v_5qR74sk4T8",
        "colab_type": "text"
      },
      "source": [
        "This snippet parses the text and cleans the string\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NWCYai4Jkw5K",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def clean_string(string):\n",
        "    #print (\"I am cleaning the string\")  \n",
        "    string=string.replace(\".\", \"\")\n",
        "    string=string.replace(\"\\n\", \"\")\n",
        "    string=string.replace(\"(\", \" \")\n",
        "    string=string.replace(\")\", \" \")\n",
        "    string=string.replace(\"[\", \" \")\n",
        "    string=string.replace(\"]\", \" \")\n",
        "    string=string.replace(\"'\", \"\")\n",
        "    string=string.replace('\"', \"\")\n",
        "    string=string.replace(\"-\",\"\")\n",
        "    string=string.replace(\",\",\"\")\n",
        "    string=string.replace(\";\",\"\")\n",
        "    string=string.lower()\n",
        "    #strip spaces from beg and end\n",
        "    string=string.strip()\n",
        "    return string    \n",
        "\n",
        "def parsexml(filename):\n",
        "  p=[]\n",
        "  h=[]\n",
        "  labels=[] \n",
        "  response = urllib.request.urlopen(filename).read()\n",
        "  root = ET.fromstring(response)\n",
        "  for premise in root.findall('pair'):\n",
        "      #print (\"I am SRijita\")\n",
        "      ptext = premise.find('t').text\n",
        "      htext = premise.find('h').text\n",
        "      label= premise.get('value')\n",
        "      \"\"\"Cleaning the text\"\"\"\n",
        "      ptext_clean=clean_string(ptext)\n",
        "      htext_clean=clean_string(htext)\n",
        "      \"\"\"Finding the tokens and storing them in array\"\"\"\n",
        "      ptext_clean_tokens=ptext_clean.split()\n",
        "      #print (ptext_clean_tokens)\n",
        "      htext_clean_tokens=htext_clean.split()\n",
        "      p.append(ptext_clean_tokens)\n",
        "      h.append(htext_clean_tokens)\n",
        "      labels.append(label)\n",
        "  #print (p[0])    \n",
        "  return p,h,labels   "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M7--0sQ4lIxn",
        "colab_type": "text"
      },
      "source": [
        "Main module to run the code"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5mWFsmqd9yiy",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        },
        "outputId": "2cab4fc0-1229-4388-aa93-ad18f6f36c1d"
      },
      "source": [
        "from __future__ import division\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.utils import to_categorical\n",
        "from tensorflow.python.keras.models import Sequential\n",
        "from tensorflow.python.keras.layers import Dense, Dropout, Activation,Bidirectional,Embedding,LSTM\n",
        "from copy import deepcopy\n",
        "from random import randint\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import ssl\n",
        "import urllib.request\n",
        "import xml.etree.ElementTree as ET\n",
        "#out_of_bag_idx=50000\n",
        "BATCH_SIZE=10\n",
        "\n",
        "trainfile=\"https://www.hlt.utdallas.edu/~moldovan/CS6320.20S/train.xml\"\n",
        "testfile=\"https://www.hlt.utdallas.edu/~moldovan/CS6320.20S/test.xml\"\n",
        "ssl._create_default_https_context = ssl._create_unverified_context\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "   vocab_dict={}\n",
        "   p_train,h_train,labels_train=parsexml(trainfile)\n",
        "   p_test,h_test,labels_test=parsexml(testfile) \n",
        "   vocab_dict,offset=populate_vocab(p_train,vocab_dict)\n",
        "   vocab_dict,offset=populate_vocab(h_train,vocab_dict,offset)\n",
        "   print (\"The length of vocab is\", len(vocab_dict))\n",
        "   train_vocab_length=len(vocab_dict)\n",
        "   vocab_dict=populate_vocab_test(p_test,vocab_dict,offset+1)\n",
        "   vocab_dict=populate_vocab_test(h_test,vocab_dict,offset+1) \n",
        "   print (\"The length of vocab is\",len(vocab_dict),offset+1)\n",
        "   #a=[]\n",
        "   #for key,value in vocab_dict.items():\n",
        "   #    a.append(value)\n",
        "   #print (max(a))      \n",
        "   \"\"\"Integer encode premise and hypotheses\"\"\"\n",
        "   ptrain=integer_encode(p_train,vocab_dict,offset+1)\n",
        "   htrain=integer_encode(h_train,vocab_dict,offset+1)\n",
        "   ptest=integer_encode(p_test,vocab_dict,offset+1)\n",
        "   htest=integer_encode(h_test,vocab_dict,offset+1)\n",
        "   #print (ptrain[0:2])\n",
        "   #print (htrain[0:2])\n",
        "   \"\"\"Integer encode the labels\"\"\"\n",
        "   labeltrain=label_encode(labels_train)\n",
        "   labeltest=label_encode(labels_test)\n",
        "   \"\"\"Finds the maxlength among premise and hypothesis\"\"\"\n",
        "   max_premise = max((x) for x in ptrain)\n",
        "   max_hypothesis=max((x) for x in htrain)\n",
        "   max_length=max(len(max_premise),len(max_hypothesis))\n",
        "   \"\"\"Padding the sequences of premise and hypothesis\"\"\"\n",
        "   ptrain_paded=pad_sequence_str(ptrain,max_length)\n",
        "   htrain_paded=pad_sequence_str(htrain,max_length)\n",
        "   ptest_paded=pad_sequence_str(ptest,max_length)\n",
        "   htest_paded=pad_sequence_str(htest,max_length)\n",
        "   \"\"\"One hot encodings of train and test labels\"\"\"\n",
        "   labeltrain=np.array(labeltrain)\n",
        "   labeltest=np.array(labeltest)\n",
        "   labeltrain_onehot = to_categorical(labeltrain)\n",
        "   labeltest_onehot = to_categorical(labeltest)\n",
        "   #print (labeltrain_onehot[0:5],labeltrain[0:5])\n",
        "   #print (labeltest_onehot[0:5])\n",
        "\n",
        "   \"\"\"Concatenate the features of premise and labels\"\"\"\n",
        "   ptrain_paded=np.array(ptrain_paded)\n",
        "   htrain_paded=np.array(htrain_paded)\n",
        "   ptest_paded=np.array(ptest_paded)\n",
        "   htest_paded=np.array(htest_paded)\n",
        "   #print (\"ptrain_paded\",ptrain_paded[5],p_train[5],vocab_dict['food'],vocab_dict['foods'])\n",
        "   #print (\"htrain_paded\",htrain_paded.shape)\n",
        "   #print (\"ptest_paded\",ptest_paded.shape)\n",
        "   #print (\"htest_paded\",htest_paded.shape)\n",
        "   train_feat=np.hstack((ptrain_paded,htrain_paded))\n",
        "   test_feat=np.hstack((ptest_paded,htest_paded))\n",
        "   print (\"The shape of the train features\", train_feat.shape)\n",
        "   print (\"The shape of the test features\", test_feat.shape)"
      ],
      "execution_count": 127,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The length of vocab is 4657\n",
            "The length of vocab is 7722 4658\n",
            "The shape of the train features (567, 54)\n",
            "The shape of the test features (800, 54)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RtEi6qYNjWPo",
        "colab_type": "text"
      },
      "source": [
        "Deep learning part of the project"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W02Y0TBejUzu",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "1287a84b-d21f-4639-d465-0638fd97d6ed"
      },
      "source": [
        "BUFFER_SIZE=train_feat.shape[0]\n",
        "\"\"\"Create tensors from data\"\"\"\n",
        "train_tensor=tf.data.Dataset.from_tensor_slices((train_feat,labeltrain_onehot))\n",
        "print (train_tensor.element_spec)\n",
        "test_tensor=tf.data.Dataset.from_tensor_slices((test_feat,labeltest_onehot))\n",
        "print (test_tensor.element_spec)\n",
        "train_batch = (train_tensor.shuffle(BUFFER_SIZE).batch(BATCH_SIZE))\n",
        "\"\"\"No shuffling for test data set\"\"\"\n",
        "test_batch = (test_tensor.batch(BATCH_SIZE))"
      ],
      "execution_count": 128,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(TensorSpec(shape=(54,), dtype=tf.int32, name=None), TensorSpec(shape=(2,), dtype=tf.float32, name=None))\n",
            "(TensorSpec(shape=(54,), dtype=tf.int32, name=None), TensorSpec(shape=(2,), dtype=tf.float32, name=None))\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4djW7t-knoWg",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 266
        },
        "outputId": "c9653962-6618-4ca0-e9ab-995811ffdd87"
      },
      "source": [
        "import time\n",
        "\"\"\"Keras model built\"\"\"\n",
        "model = Sequential()\n",
        "model.add(Embedding(input_dim=train_vocab_length+2,output_dim=25))\n",
        "model.add(Bidirectional(LSTM(25)))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Dense(2, activation='sigmoid'))\n",
        "model.compile(optimizer='adam',loss='binary_crossentropy', metrics=['accuracy'])\n",
        "# Train model on your dataset\n",
        "model.fit(train_batch,epochs=5)\n",
        "start_time=time.time()\n",
        "score = model.evaluate(test_batch)\n",
        "print (\"The throughput for inference time in seconds %\",  (time.time() - start_time))\n",
        "print (\"The score is\",score)\n",
        "ypred=model.predict_classes(test_batch)\n",
        "#print (ypred[0:20])"
      ],
      "execution_count": 129,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/5\n",
            "57/57 [==============================] - 1s 12ms/step - loss: 0.6933 - accuracy: 0.5044\n",
            "Epoch 2/5\n",
            "57/57 [==============================] - 1s 12ms/step - loss: 0.6759 - accuracy: 0.7443\n",
            "Epoch 3/5\n",
            "57/57 [==============================] - 1s 12ms/step - loss: 0.4780 - accuracy: 0.8201\n",
            "Epoch 4/5\n",
            "57/57 [==============================] - 1s 13ms/step - loss: 0.1965 - accuracy: 0.9400\n",
            "Epoch 5/5\n",
            "57/57 [==============================] - 1s 13ms/step - loss: 0.1312 - accuracy: 0.9612\n",
            "80/80 [==============================] - 0s 6ms/step - loss: 1.3032 - accuracy: 0.5200\n",
            "The throughput for inference time in seconds % 1.2267887592315674\n",
            "The score is [1.3031561374664307, 0.5199999809265137]\n",
            "[0 1 0 0 1 1 0 1 1 0 0 1 1 1 0 1 1 1 1 1]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eJaN5oOl8pjH",
        "colab_type": "text"
      },
      "source": [
        "Code to print the metrics; Accuracy, Recall,F1-score"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UPTc6kCu8nfp",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        },
        "outputId": "f50ef532-1200-44b7-a837-a4e07c85c3a4"
      },
      "source": [
        "from sklearn.metrics import recall_score\n",
        "from sklearn.metrics import f1_score\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import precision_score\n",
        "\n",
        "print (\"The accuracy is\",accuracy_score(labeltest, ypred))\n",
        "print (\"The Recall is\",recall_score(labeltest, ypred))\n",
        "print (\"The f1-score is\",f1_score(labeltest, ypred))\n",
        "print (\"The precision-score is\",precision_score(labeltest, ypred))"
      ],
      "execution_count": 130,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The accuracy is 0.52\n",
            "The Recall is 0.69\n",
            "The f1-score is 0.5897435897435898\n",
            "The precision-score is 0.5149253731343284\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ySfoyAebEqUS",
        "colab_type": "text"
      },
      "source": [
        "Save the learnt model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OJ6SDt9lEl0y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Saving the model\n",
        "model_json = model.to_json()\n",
        "with open(\"model.json\", \"w\") as ofile:\n",
        "    ofile.write(model_json)\n",
        "model.save_weights(\"model.h5\")"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}